{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape for the VGG19 model\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Create an instance of the VGG19 model with pre-trained weights\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the pre-trained layers so they are not trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top of the pre-trained base model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # Set the number of output classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and create data generators\n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_global = 42\n",
    "epoch_val = 1000\n",
    "batch_size_val = 32\n",
    "threshold_val = 1e-4\n",
    "IMG_SIZE = (224, 224)\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Define the paths for train and test data\n",
    "train_dir =  'chest_xray/train'\n",
    "test_dir =  'chest_xray/test'\n",
    "val_dir =  'chest_xray/val'\n",
    "\n",
    "# Preprocess the data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5102 files belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\ankit\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "<MapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "Found 624 files belonging to 2 classes.\n",
      "Found 130 files belonging to 2 classes.\n",
      "\n",
      "class names are ['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into train and validation sets\n",
    "train_generator = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                        label_mode=\"categorical\",\n",
    "                                                        image_size=IMG_SIZE,\n",
    "                                                        seed=random_state_global,\n",
    "                                                        batch_size = batch_size_val,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "train_ds = train_generator.map(lambda x, y: (normalization_layer(x), y))\n",
    "print(train_ds)\n",
    "\n",
    "test_generator = tf.keras.utils.image_dataset_from_directory(test_dir,\n",
    "                                                        label_mode=\"categorical\",\n",
    "                                                        image_size=IMG_SIZE,\n",
    "                                                        seed=random_state_global,\n",
    "                                                        batch_size = batch_size_val,\n",
    "                                                        shuffle=False) # don't shuffle test data for prediction analysis\n",
    "validation_generator = tf.keras.utils.image_dataset_from_directory(val_dir,\n",
    "                                                        label_mode=\"categorical\",\n",
    "                                                        image_size=IMG_SIZE,\n",
    "                                                        seed=random_state_global,\n",
    "                                                        batch_size = batch_size_val,\n",
    "                                                        shuffle=False) # don't shuffle valid data for prediction analysis\n",
    "\n",
    "class_names = validation_generator.class_names\n",
    "print(f'\\nclass names are {class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# my callbacks\n",
    "my_callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1, min_delta=1e-4,mode=\"min\",restore_best_weights=False,verbose=1)]\n",
    "# Train the model\n",
    "history_1 = model.fit(train_ds,\n",
    "                        epochs=epoch_val,\n",
    "                        validation_data=validation_generator,\n",
    "                        callbacks = my_callbacks,\n",
    "                        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 85s 4s/step - loss: 15.0949 - accuracy: 0.7933\n",
      "Test loss: 15.094948768615723\n",
      "Test accuracy: 0.7932692170143127\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
